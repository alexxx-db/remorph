---
sidebar_position: 1
title: Profiler Guide
---
import Admonition from '@theme/Admonition';

# Profiler Guide

## Overview

<p>
The **Lakebridge Profiler** is designed to extract and analyze metadata from database systems, providing insights into your source environment.
The profiler helps you understand system configurations, resource utilization, query patterns, and performance metrics to aid in migration planning.
</p>

Key capabilities:
- **Database Metadata Extraction**: Captures schema information, table structures, and object definitions
- **Performance Analytics**: Collects query execution metrics and resource utilization data
- **Workload Analysis**: Profiles active queries and identifies optimization opportunities

<Admonition type="info" title="Prerequisites">
    For detailed source system specific prerequisites, refer to <a href="./Prerequisites" style={{ fontWeight: 'bold', color: '#1976d2', textDecoration: 'underline' }}>Pre-Req</a> section.
</Admonition>

## Configure Profiler

Before running the profiler, you need to configure the connection details for your source system.

Execute the following command to configure the profiler:

```bash
databricks labs lakebridge configure-database-profiler
```

This will prompt you to select the source system and provide connection details:

```console
Please select the source system you want to configure
[0] mssql
[1] synapse
Enter a number between 0 and 1: 1
(local | env)
local means values are read as plain text
env means values are read from environment variables fall back to plain text if not variable is not found

Enter secret vault type (local | env)
[0] env
[1] local
Enter a number between 0 and 1: 1
Please provide Synapse Workspace settings:
Enter Synapse workspace name: synapse
Enter SQL user: user
Enter SQL password:
Enter timezone (e.g. America/New_York) (default: UTC):
Enter the ODBC driver installed locally (default: ODBC Driver 18 for SQL Server):
Please provide Azure access settings:
Enter development endpoint: synapse.endpoint
Please select JDBC authentication type:
Select authentication type
[0] ad_passwd_authentication
[1] spn_authentication
[2] sql_authentication
Enter a number between 0 and 2: 2
Enter fetch size (default: 1000):
Enter login timeout (seconds) (default: 30):
Exclude serverless SQL pool from profiling? (default: no):
Exclude dedicated SQL pools from profiling? (default: no):
Exclude Spark pools from profiling? (default: no):
Exclude monitoring metrics from profiling? (default: no):
Redact SQL pools SQL text? (default: no):
```

## Execute Profiler

Once configured, run the profiler to extract metadata and performance metrics from your source system:

```bash
databricks labs lakebridge execute-database-profiler --help
```

output:

```console
Profile the source system database

Usage:
  databricks labs lakebridge execute-database-profiler [flags]

Flags:
  -h, --help                 help for execute-database-profiler
      --source-tech string   (Optional) The technology/platform of the sources to Profile

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)
```

The profiler will:
1. Connect to your source system using the configured credentials
2. Execute the profiling pipeline to extract metadata and metrics
3. Store the results in the configured output location
4. Generate a summary report of the profiling execution

:::tip
The profiler can be run multiple times to capture different time periods or updated configurations.
Each execution will create a timestamped snapshot of your source environment.
:::

## Supported Source Systems

| Source Platform | Configuration Status |
|:---------------:|:-------------------:|
| Azure Synapse | &#x2705; |

## Publish Profiler Summary Dashboard

Upload a summary of a profiler run as a dashboard to your local Databricks workspace using the `create-profiler-dashboard`.

### Description

`create-profiler-dashboard` command converts a profiler extract file to a dashboard and deploys it to a Databricks workspace.
This helps users quickly visualize and explore profiler results without additional setup in a SQL or BI tool.

**Note:** This command is part of the experimental profiler workflow and is subject to change in future versions.

### Syntax

```bash
databricks labs lakebridge create-profiler-dashboard \
  --extract-file <path-to-local-extract-file> \
  --source-tech <source-system-name> \
  --uc-volume <uc-volume-path> \
  [--catalog-name <uc-catalog-name>] \
  [--schema-name <uc-schema-name>]
```

### Options

#### `--extract-file`

**Description:**
Specifies the local file path to an extract file containing the profiler results.
This file is automatically output after the successful execution of a profiler run using
`databricks labs lakebridge execute-database-profiler`.

**Example:**

```bash
--extract-file ./output/profiler_results.db
```

**Required:** Yes

#### `--source-tech`

**Description:**

Specifies the name of the source system technology that was profiled.
This value is used to load the profiler dashboard template.

**Example:**

```bash
--source-tech synapse
```

**Required:** Yes

#### `--uc-volume`

**Description:**

The name of the Unity Catalog (UC) volume where the extract file will be uploaded.

**Example:**

```bash
--uc-volume /Volumes/lakebridge_profiler/profiler_runs
```

**Required:** Yes

#### `--catalog-name`

**Description:**

(Optional) The name of the catalog where the extract data will be uploaded to as Delta tables.
If not provided, the command uses the default catalog `lakebridge_profiler`.

**Example:**

```bash
--catalog-name lakebridge_profiler
```

**Required:** No

#### `--schema-name`

**Description:**

(Optional) The name of the schema where extract data will be uploaded as Delta tables.
If not provided, the command uses the default schema `profiler_runs`.

**Example:**

```bash
--schema-name profiler_runs
```

**Required:** No

### Example

The following example deploys a profiler summary dashboard for an Azure Synapse profiler run:

```bash
databricks labs lakebridge create-profiler-dashboard \
  --extract-file ./output/profile_output.db \
  --source-tech synapse \
  --uc-volume /Volumes/lakebridge_profiler/profiler_runs \
  --catalog-name lakebridge_profiler \
  --schema-name profiler_runs
```

#### Result:

```bash
Profiler extract file was uploaded successfully.
Dashboard created at: https://<databricks-workspace-url>/sql/dashboards/<dashboard-id>
```
#### Output

When the command executes, the following actions will take place:

1. The profiler extract file is uploaded to the specified UC volume.
2. A Databricks job for ingesting the extract file is deployed to the Databricks workspace and immediately executed.
3. The profiler extract results are converted to Delta tables in the workspace catalog and schema.
4. A Databricks dashboard summarizing the profiler results is deployed to the workspace.
5. A workspace URL for accessing the dashboard is returned.
