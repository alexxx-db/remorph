---
sidebar_position: 4
title: Transpile Guide
---
import useBaseUrl from '@docusaurus/useBaseUrl';

#  Transpile Guide

## Supported dialects

<table>
  <thead>
    <tr>
      <th>Source system type</th>
      <th>Source Technology</th>
      <th>Source System</th>
      <th>BladeBridge</th>
      <th>Morpheus</th>
      <th>Switch (Experimental)</th>
    </tr>
  </thead>
  <tbody>
    <!-- SQL -->
    <tr>
      <td rowspan="9">SQL</td>
      <td>`mssql`</td>
      <td>MMicrosoft SQL Server, Azure SQL Database, Azure SQL Managed Instance, Amazon RDS for SQL Server</td>
      <td>DBSQL</td>
      <td>DBSQL</td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`mysql`</td>
      <td>MySQL, MariaDB, and MySQL-compatible services (including Amazon Aurora MySQL, RDS, Google Cloud SQL)</td>
      <td></td>
      <td></td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`netezza`</td>
      <td>IBM Netezza</td>
      <td>DBSQL</td>
      <td></td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`oracle`</td>
      <td>Oracle Database, Oracle Exadata, and Oracle-compatible services (including Amazon RDS)</td>
      <td>DBSQL</td>
      <td></td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`postgresql`</td>
      <td>PostgreSQL and PostgreSQL-compatible services (including Amazon Aurora PostgreSQL, RDS, Google Cloud SQL)</td>
      <td></td>
      <td></td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`redshift`</td>
      <td>Amazon Redshift</td>
      <td>DBSQL (experimental)</td>
      <td></td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`snowflake`</td>
      <td>Snowflake (including dbt Repointing)</td>
      <td></td>
      <td>DBSQL</td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`synapse`</td>
      <td>Azure Synapse Analytics (dedicated SQL pools)</td>
      <td>DBSQL</td>
      <td>DBSQL</td>
      <td>SparkSql</td>
    </tr>
    <tr>
      <td>`teradata`</td>
      <td>Teradata</td>
      <td>DBSQL</td>
      <td></td>
      <td>SparkSql</td>
    </tr>

    <!-- ETL -->
    <tr>
      <td rowspan="2">ETL</td>
      <td>`datastage`</td>
      <td>IBM DataStage</td>
      <td>SparkSql, PySpark</td>
      <td></td>
      <td>SDP</td>
    </tr>
    <tr>
      <td>`ssis`</td>
      <td>SSIS</td>
      <td>SparkSql (experimental)</td>
      <td></td>
      <td>SDP</td>
    </tr>

    <!-- Orchestration -->
    <tr>
      <td>Orchestration</td>
      <td></td>
      <td>Airflow</td>
      <td></td>
      <td></td>
      <td>Databricks Workflow</td>
    </tr>

    <!-- Generic -->
    <tr>
      <td rowspan="2">Generic</td>
      <td></td>
      <td>Python Code</td>
      <td></td>
      <td></td>
      <td>Python Notebook</td>
    </tr>
    <tr>
      <td></td>
      <td>Scala Code</td>
      <td></td>
      <td></td>
      <td>Python Notebook</td>
    </tr>
  </tbody>
</table>

## Verify Installation
Verify the successful installation by executing the provided command; confirmation of a successful installation is indicated when the displayed output aligns with the example screenshot provided:

Command:
```shell
databricks labs lakebridge transpile --help
```

Should output:
```console
Transpile SQL/ETL sources to Databricks-compatible code

Usage:
  databricks labs lakebridge transpile [flags]

Flags:
      --catalog-name name             (Optional) Catalog name, only used when validating converted code
      --error-file-path path          (Optional) Local path where a log of conversion errors (if any) will be written
  -h, --help                          help for transpile
      --input-source path             (Optional) Local path of the sources to be convert
      --output-folder path            (Optional) Local path where converted code will be written
      --schema-name name              (Optional) Schema name, only used when validating converted code
      --skip-validation string        (Optional) Whether to skip validating the output ('true') after conversion or not ('false')
      --source-dialect string         (Optional) The source dialect to use when performing conversion
      --transpiler-config-path path   (Optional) Local path to the configuration file of the transpiler to use for conversion

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)
```

## Execution Pre-Set Up
When you run `install-transpile`, you will be prompted for settings to use when transpiling your sources. You can
choose to provide these at the time of installation, or to provide them later as arguments when transpiling.

The `transpile` command will trigger the conversion of the specified code. These settings provided during `install-transpile` can be overridden (or provided if unavailable) using the command-line options:
 - `input-source`: The local filesystem path to the sources that should be transpiled. This must be provided if not set during `install-transpile`.
 - `output-folder`: The local filesystem path where converted code will be written. This must be provided if not set during `install-transpile`.
 - `source-dialect`: Dialect name (ex: snowflake, oracle, datastage, etc). This must be provided if not set during `install-transpile`.
 - `overrides-file`: An optional local path to a JSON file containing custom overrides for the transpilation process, if the underlying transpiler supports this. (Refer to [this documentation](pluggable_transpilers/bladebridge/bladebridge_configuration) for more details on custom overrides.)
 - `target-technology`: The target technology to use for conversion output. This must be provided if not set during `install-transpile` and the underlying transpiler requires it for the source dialect in use.
- `error-file-path`: The path to the file where a log of conversion errors will be stored. If not provided here or during `install-transpile` no error log will be written.
 - `skip-validation`: Whether the transpiler will skip the validation of transpiled SQL sources. If not provided here or during `install-transpile` validation will be attempted by default.
 - `catalog-name`: The name of the catalog in Databricks to use when validating transpiled SQL sources. If not specified, `remorph` will be used as the default catalog.
 - `schema-name`: The name of the schema in Databricks to use when validating transpiled SQL sources. If not specified, `transpiler` will be used as the default schema.
 - `transpiler-config-path`: This path of the configuration file for the transpiler to use for conversion. This is normally inferred from the source dialect or chosen during `install-transpile` if multiple transpilers support the source dialect.

## Execution
Execute the below command to initialize the transpile process passing the arguments to the command directly in the call.
```shell
databricks labs lakebridge transpile --transpiler-config-path <absolute-path> --input-source <absolute-path> --source-dialect <snowflake> --output-folder <absolute-path> --skip-validation <True|False> --catalog-name <catalog name> --schema-name <schema name>
```
<img src={useBaseUrl('img/transpile-run.gif')} alt="transpile-run" />
<br />
<br />
If you have configured all the required inputs at installation time, you can simply run:
```bash
databricks labs lakebridge transpile
```
